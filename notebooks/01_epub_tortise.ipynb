{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wassname/miniforge3/envs/tts/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"modified from https://gist.github.com/endes0/0967d7c5bb1877559c4ae84be05e036c\"\"\"\n",
    "from tika import parser\n",
    "\n",
    "import torchaudio\n",
    "import argparse\n",
    "from sanitize_filename import sanitize\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from tortoise.api import TextToSpeech\n",
    "from tortoise.utils.audio import load_audio, load_voice, load_voices\n",
    "from tortoise.utils.tokenizer import VoiceBpeTokenizer\n",
    "\n",
    "import torch\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "# import pysbd\n",
    "from typing import List\n",
    "from loguru import logger\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class Writer:\n",
    "    out_dir: Path\n",
    "    # tts: TTS\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.m3u = open(self.out_dir / 'playlist.m3u', 'w')\n",
    "        self.m3u.write('#EXTM3U\\n')\n",
    "        self.chapter = 1\n",
    "\n",
    "    def write_chapter(self, waveforms: torch.tensor, SAMPLE_RATE=24000):\n",
    "        wav_f = self.out_dir / f'{self.chapter}.ogg'\n",
    "        torchaudio.save(wav_f, waveforms.cpu(), SAMPLE_RATE)\n",
    "        self.m3u.write(f'{wav_f}\\n')\n",
    "        self.chapter += 1\n",
    "        return wav_f\n",
    "\n",
    "\n",
    "    def close(self):\n",
    "        self.m3u.close()\n",
    "\n",
    "def split_into_sentences(text, tokenizer) -> List[str]:        \n",
    "    limit = 200\n",
    "    chunk_limit = limit\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        length_function=lambda x: len(tokenizer.encode(x)),\n",
    "        chunk_size=chunk_limit,\n",
    "        chunk_overlap=0,\n",
    "        keep_separator=True,\n",
    "        strip_whitespace=True,\n",
    "        separators=[\n",
    "            \"\\n\\n\", \"\\n\", \"\\xa0\", '<div>', '<p>', '<br>', \"\\r\", \".\",  \"!\", \"?\", \n",
    "            '\"', \"'\", \"‘\", \"’\", \"“\", \"”\", \"„\", \"‟\",  \n",
    "            \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \n",
    "            \"…\", \":\", \";\", \"—\", \"   \"\n",
    "            \" \", '' # these ensure that there is always something to split by so chunks are always at limit\n",
    "    ],\n",
    "    )\n",
    "    texts = splitter.split_text(text)\n",
    "    ls = [splitter._length_function(x) for x in texts]\n",
    "    logger.debug(f'split lengths {ls}. max={max(ls)} chunk_limit={chunk_limit}')\n",
    "    assert all([l<=limit for l in ls]), 'all senteces should be below limit'\n",
    "    return texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/media/wassname/SGIronWolf/projects5/tts-ai/use-tts-mjc')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__file__ = '../01_epub_tortise.ipynb'\n",
    "root_dir = Path(__file__).resolve().absolute().parent\n",
    "root_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-08 11:19:11.630\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mOutput folder: /media/wassname/SGIronWolf/projects5/tts-ai/use-tts-mjc/out/a_short_guide_to_the_inner_citadel_-_massimo_pigliucci20231008_03-19-11\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Get the command line arguments\n",
    "parser2 = argparse.ArgumentParser()\n",
    "parser2.add_argument('--epub', type=Path, \n",
    "                     default=root_dir/'data/A Short Guide to the Inner Citadel - Massimo Pigliucci.epub',\n",
    "                    #  default=root_dir/'data/golden_saying_of_epictetus.epub',\n",
    "                    help='PDF file to read')\n",
    "parser2.add_argument('-o', '--out', type=Path, default=None, help='Output folder')\n",
    "parser2.add_argument('-f', '--force', action='store_true', default=False, help='Overwrite')\n",
    "parser2.add_argument('-t', '--test', action='store_true', default=False, help='Overwrite')\n",
    "parser2.add_argument('-l', '--limit', type=int, default=400,\n",
    "                    help='Maximum number of characters to synthesize at once')\n",
    "parser2.add_argument('-m', '--model', type=str, \n",
    "                    default=\"tts_models/multilingual/multi-dataset/xtts_v1\",\n",
    "                    # default='facebook/fastspeech2-en-ljspeech',\n",
    "                    help='fairseq model to use from HuggingFace Hub')\n",
    "parser2.add_argument('-s', '--speaker', type=Path, default=root_dir / \"data/speakers/donaldrobertson.wav\",\n",
    "                    help='Speaker wav to use from the model')\n",
    "args = parser2.parse_args([])\n",
    "\n",
    "if args.out is None:\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.utcnow().strftime('%Y%m%d_%H-%M-%S')\n",
    "    args.out = root_dir / 'out' / (sanitize(args.epub.stem).replace(' ', '_').lower() + timestamp)\n",
    "\n",
    "# load epib\n",
    "parsed = parser.from_file(str(args.epub))\n",
    "text = parsed[\"content\"]\n",
    "if args.test:\n",
    "    text = text[:1000]\n",
    "\n",
    "\n",
    "# make output directory\n",
    "out_dir = Path(args.out)\n",
    "if out_dir.exists():\n",
    "    if not args.force:\n",
    "        logger.warning('Output folder already exists. Use -f to overwrite.')\n",
    "        exit(1)\n",
    "    else:\n",
    "        for f in out_dir.glob('*'):\n",
    "            f.unlink()\n",
    "        out_dir.rmdir()\n",
    "out_dir.mkdir()\n",
    "logger.info(f'Output folder: {out_dir}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-08 11:19:11.774\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1muse_cuda True\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-08 11:19:20,690] [INFO] [logging.py:93:log_dist] [Rank -1] DeepSpeed info: version=0.8.3, git-hash=unknown, git-branch=unknown\n",
      "[2023-10-08 11:19:20,691] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead\n",
      "[2023-10-08 11:19:20,692] [INFO] [logging.py:93:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
      "Installed CUDA version 11.5 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/wassname/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/wassname/.cache/torch_extensions/py310_cu117/transformer_inference/build.ninja...\n",
      "Building extension module transformer_inference...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "Time to load transformer_inference op: 0.051093101501464844 seconds\n",
      "[2023-10-08 11:19:21,249] [INFO] [logging.py:93:log_dist] [Rank -1] DeepSpeed-Inference config: {'layer_id': 0, 'hidden_size': 1024, 'intermediate_size': 4096, 'heads': 16, 'num_hidden_layers': -1, 'fp16': True, 'pre_layer_norm': True, 'local_rank': -1, 'stochastic_mode': False, 'epsilon': 1e-05, 'mp_size': 1, 'q_int8': False, 'scale_attention': True, 'triangular_masking': True, 'local_attention': False, 'window_size': 1, 'rotary_dim': -1, 'rotate_half': False, 'rotate_every_two': True, 'return_tuple': True, 'mlp_after_attn': True, 'mlp_act_func_type': <ActivationFuncType.GELU: 1>, 'specialized_mode': False, 'training_mp_size': 1, 'bigscience_bloom': False, 'max_out_tokens': 1024, 'scale_attn_by_inverse_layer_idx': False, 'enable_qkv_quantization': False, 'use_mup': False, 'return_single_tuple': False}\n",
      "Installed CUDA version 11.5 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination\n",
      "Time to load transformer_inference op: 0.0019137859344482422 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module transformer_inference...\n",
      "Using /home/wassname/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module transformer_inference, skipping build step...\n",
      "Loading extension module transformer_inference...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# write metadata to dir\n",
    "from json_tricks import dump, dumps, load, loads, strip_comments\n",
    "f_metadata = out_dir / 'metadata.json'\n",
    "with open(f_metadata, 'w') as fo:\n",
    "    dump(dict(\n",
    "        epub_metadata=parsed['metadata'],\n",
    "        args=args.__dict__,\n",
    "        \n",
    "    ), fo, indent=4)\n",
    "\n",
    "# should be torch tensors containing 22.05kHz waveform data.\n",
    "# see https://github.com/neonbjb/tortoise-tts/blob/5bbb0e0b97ea2f62c12e90402e8ad4faee55e697/tortoise/api.py#L365C82-L365C140\n",
    "ref, INPUT_SAMPLE_RATE = torchaudio.load(args.speaker)\n",
    "reference_clips = [ref[..., -400000:]] # take just the last ~12 seconds\n",
    "\n",
    "# load model\n",
    "use_cuda = False if args.test else torch.cuda.is_available()\n",
    "logger.info(f'use_cuda {use_cuda}')\n",
    "\n",
    "\n",
    "tts = TextToSpeech(use_deepspeed=True, kv_cache=True, half=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SAMPLE_RATE\n",
    "OUTPUT_SAMPLE_RATE = 24000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-08 11:19:24.549\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msplit_into_sentences\u001b[0m:\u001b[36m41\u001b[0m - \u001b[34m\u001b[1msplit lengths [22, 185, 169, 140, 123, 147, 18, 172, 30, 137, 112, 157, 174, 157, 104, 124, 125, 138, 23, 194, 120, 171, 197, 5, 126, 124, 134, 129, 166, 135, 174, 186, 130, 75, 75, 115, 97, 73, 73, 163, 114, 93, 143, 94, 147, 130, 24, 169, 142, 133, 73, 192, 136, 134, 73, 131, 89, 31, 60, 165, 1, 175, 120, 162, 96, 1, 189, 174, 59, 92, 163, 46, 150, 86, 176, 25, 196, 18, 124, 177, 139, 143, 96, 170, 51, 175, 191, 156, 186, 171, 99, 108, 17, 189, 39, 20, 144, 140, 161, 96, 82, 123, 187, 106, 116, 84, 194, 191, 110, 117, 184, 104, 140, 102, 155, 1, 197, 80, 95, 198, 191, 129, 193, 177, 113, 116, 144, 143, 158, 118, 124, 32, 190, 171, 158, 78, 148, 58, 152, 102, 135, 55, 177, 136, 138, 182, 24, 76, 158, 121, 154, 165, 172, 67, 104, 119, 123, 157, 189, 105, 43, 170, 58, 168, 190, 137, 199, 163, 41, 111, 17, 186, 112, 199, 170, 183, 149, 156, 131, 88, 160, 163, 112, 107, 178, 179, 125, 159, 199, 12, 150, 133, 148, 181, 168, 138, 179, 186, 49, 68, 178, 74, 119, 98, 121, 172, 108, 95, 28, 193, 169, 145, 184, 78, 196, 22, 115, 97, 174, 179, 129, 117, 155, 197, 116, 92, 168, 146, 117, 177, 80, 165, 1, 172, 125, 124, 145, 105, 195, 166, 149, 166, 135, 150, 179, 63, 57, 143, 102, 186, 24, 187, 1, 103, 105, 166, 112, 191, 24, 187, 144, 112, 34, 65, 170, 62, 76, 144, 137, 15, 101, 157, 108, 137, 76, 152, 177, 167, 189, 13, 120, 163, 151, 172, 165, 88, 180, 80, 196, 165, 177, 65, 148, 147, 85, 67, 138, 160, 109, 47, 192, 62, 93, 174, 139, 75, 134, 173, 142, 187, 17, 49, 167, 123, 159, 111, 135, 123, 113, 168, 157, 126, 186, 35, 93, 132, 197, 25, 194, 157, 198, 134, 190, 167, 67, 114, 171, 139, 175, 171, 147, 39, 163, 102, 98, 26, 137, 99, 18, 128, 154, 141, 63, 127, 150, 116, 52, 84, 189, 17, 145, 168, 160, 173, 183, 191, 196, 128, 147, 142, 129, 71, 116, 124, 167, 196, 135, 160, 79, 154, 63, 9, 163, 193, 27, 10, 191, 69, 18]. max=199 chunk_limit=200\u001b[0m\n",
      "chunks:   0%|          | 0/389 [00:00<?, ?it/s]\u001b[32m2023-10-08 11:19:24.551\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[34m\u001b[1mcurrent sentence `A Short Guide to the Inner Citadel`\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating autoregressive samples..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Free memory : 8.120789 (GigaBytes)  \n",
      "Total memory: 10.731750 (GigaBytes)  \n",
      "Requested memory: 1.687500 (GigaBytes) \n",
      "Setting maximum total tokens (input + output) to 1024 \n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:03<00:00,  3.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing best candidates using CLVP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00,  9.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming autoregressive outputs into audio..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:02<00:00, 27.28it/s]\n",
      "chunks:   0%|          | 1/389 [00:10<1:09:21, 10.72s/it]\u001b[32m2023-10-08 11:19:35.276\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[34m\u001b[1mcurrent sentence `A Short Guide to The Inner Citadel On Pierre Hadot’s Classic Analysis of Marcus Aurelius’ Meditations By Massimo Pigliucci   © Massimo Pigliucci, 2021   A Short Guide to The Inner Citadel — On Pierre Hadot’s Classic Analysis of Marcus Aurelius’ Meditations`\u001b[0m\n",
      "chunks:   1%|          | 2/389 [01:17<4:41:40, 43.67s/it]\u001b[32m2023-10-08 11:20:42.010\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[34m\u001b[1mcurrent sentence `By Massimo Pigliucci, K.D. Irani Professor of Philosophy, the City College of New York    Stoa Nova Publications   Cover: Pierre Hadot, Wikipedia   If you like this free booklet, please consider supporting my writings at Patreon or Medium figsinwinter.blog`\u001b[0m\n",
      "chunks:   1%|          | 3/389 [02:17<5:27:45, 50.95s/it]\u001b[32m2023-10-08 11:21:41.613\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[34m\u001b[1mcurrent sentence `Introduction   Pierre Hadot’s The Inner Citadel is a classic of modern Stoicism and a must read for anyone seriously interested in the philosophies of Marcus Aurelius and of the thinker who influenced him the most, Epictetus.`\u001b[0m\n",
      "chunks:   1%|          | 4/389 [03:05<5:21:03, 50.04s/it]\u001b[32m2023-10-08 11:22:30.252\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[34m\u001b[1mcurrent sentence `This publication emerged out of an intensive three-day workshop I taught on Hadot’s book and represents an attempt to introduce readers to what is otherwise a fairly technical and lengthy treatment.`\u001b[0m\n",
      "chunks:   1%|▏         | 5/389 [03:48<5:03:23, 47.40s/it]\u001b[32m2023-10-08 11:23:12.991\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[34m\u001b[1mcurrent sentence `Nevertheless, please take what you are about to read as an invitation and a guide to actually reading Hadot, not just as self-sufficient Cliffs Notes.   Enjoy, study, and practice.   —Massimo Pigliucci  Brooklyn, Summer 2021`\u001b[0m\n",
      "chunks:   2%|▏         | 6/389 [04:53<5:41:07, 53.44s/it]\u001b[32m2023-10-08 11:24:18.148\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[34m\u001b[1mcurrent sentence `1-Marcus Aurelius’ teachers`\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tts.tokenizer\n",
    "segs = split_into_sentences(text, tokenizer)\n",
    "waveforms = []\n",
    "writer = Writer(out_dir)\n",
    "for i, t in enumerate(tqdm(segs, desc='chunks')):\n",
    "    t = t.replace('\\n', ' ').strip()\n",
    "    # Skip empty text\n",
    "    if t == None or t == '':\n",
    "        continue\n",
    "    # check if contains words or numbers\n",
    "    if not re.search('[a-zA-Z0-9]', t):\n",
    "        logger.debug(f'Skipping text without words or numbers `{t}`')\n",
    "        continue\n",
    "    logger.debug(f'current sentence `{t}`')\n",
    "    \n",
    "    wav_t = tts.tts_with_preset(t, voice_samples=reference_clips, preset='fast', verbose=i==0) # ultra_fast, fast, standard\n",
    "    wav = wav_t.cpu()\n",
    "    waveforms.append(wav)\n",
    "    \n",
    "    len_wav = sum([w.shape[-1] for w in waveforms])\n",
    "    if len_wav > 10000000//4:  # ~20G of RAM, ~2 minutes of audio output, ~7 minutes to generate\n",
    "        wavs = torch.concat(waveforms, dim=-1).cpu().squeeze(0)\n",
    "        wav_f = writer.write_chapter(wavs, OUTPUT_SAMPLE_RATE)\n",
    "        logger.warning(f\"wrote chapter {wav_f}\")\n",
    "        waveforms = []\n",
    "        \n",
    "if len(waveforms):  \n",
    "    writer.write_chapter(waveforms)\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "len_wav = sum([w.shape[-1] for w in waveforms])\n",
    "print(len_wav)\n",
    "\n",
    "wavs = torch.concat(waveforms, dim=-1).cpu().squeeze(0)\n",
    "writer.write_chapter(wavs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
